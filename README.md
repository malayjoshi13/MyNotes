## NLP

1. Architectures <br>
1.1. [LSTM (a type of RNN)](https://docs.google.com/document/d/1al-YFsSfIWcVt9nRIfFREqJVszUKPduPXgtNyuxLaZA/edit?usp=sharing) <br>
1.2. Transformer (a type of Language model) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.2.1. [Vanilla Transformer](https://docs.google.com/document/d/1iM8tG3rspHcS-D23kZVo71uP3Qkm9HUyAOAXesK7rgc/edit?usp=sharing) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.2.2. [Transformer-based pre-trained LMs for machine translation](https://docs.google.com/document/d/1okp-PTvV5kAsaFU6yEpyZLIzcN9yDKP9WwesKo_uHw0/edit?usp=sharing) <br>
1.3. BERT (a type of Language model) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.3.1. [Original BERT](https://docs.google.com/document/d/1QzvKquupoa3m-oZXYKaf-Zp189hQoyp1oiDh5eZgjcs/edit?usp=drive_link) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.3.2. [BioBERT](https://docs.google.com/document/d/1YWCwVeTFuEtk6hyeW-iKhRIJGiyd6ZUqzBAh6R6ckW4/edit?usp=sharing) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.3.2. [RoBERTa](https://docs.google.com/document/d/12_b7DXJf7bDoZGO1IpvuKBYI7NGaTcg3PWaywq4Po3o/edit?usp=sharing) <br>

2. Word Embedding Models (https://www.linkedin.com/posts/amanc_artificialintelligence-machinelearning-deeplearning-activity-7152142197998424064-gxIh?utm_source=share&utm_medium=member_android)<br>
2.1. Context-independent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.1.1. Context-independent without ML <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - BOW<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - TF-IDF<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.1.2. Context-independent with ML <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [Word2Vec](https://docs.google.com/document/d/1uMI2jRvtdNcC7F-c9de-xiXpc__-u0TOcJnucnup-Vc/edit?usp=sharing) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [GloVe](https://docs.google.com/document/d/1h-s2ePP7vvNtX0noQXvpXr6_oTeKCFPdFmKBGq-y7ho/edit?usp=sharing) <br>
2.2. Context-independent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.1. Context-independent RNN-based <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - ELMO<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - CoVe<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.2. Context-independent Transformer-based <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - BERT<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - RoBERTa<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - ALBERT<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - XLM<br>
  
5. Decoding algorithms <br>
3.1 [Greedy search vs Beam search](https://docs.google.com/document/d/1JrVWp7wnZP2rT4xRL-KTCtDI5hdW_jniup_iFwdP3To/edit?usp=sharing) <br>

6. Metrics <br>
4.1. [BLEU score](https://docs.google.com/document/d/1lKH2x3n77tTvh3Jfe6sV-VisUQZ68Q0YaD3WQSW13kk/edit?usp=sharing) <br>
4.2. [ROUGE score](https://docs.google.com/document/d/1xUQj_GsOtHkqW8wq5NAYLoqvkfCqhXfFGsAqX9ZEsag/edit?usp=sharing) <br>
4.3. [Character Error Rate (CER) & Word Error Rate (WER)](https://docs.google.com/document/d/1XbUNDnR6FfuDVmdKZopTqz5a5UWmyCzfso-Hn73p7Ro/edit?usp=drive_link)



## Computer Vision 

1. [Convolutional Neural Networks](https://docs.google.com/document/d/1vEKRSIQn7QWPnfpE1zpILDuuzjyD0k33RLU7N--niSM/edit?usp=sharing) -- arch, working, accuracies <br>
2. [Background extraction algorithms](https://docs.google.com/document/d/1XEhd3OpThcv1u9DNhny6Gjk2W1ihsylnhBf5Q4bcbks/edit?usp=sharing) <br>
3. [Face detection, alignment, and recognition models](https://docs.google.com/document/d/1trH2mB0tfAA9cAvHFnjXmSHT_wx5Xs4iKmy7ShQOOw8/edit?usp=sharing) <br>
4. [Similarity metrics for finding similarity between image representation vectors](https://docs.google.com/document/d/1BU7CyktUr5x3UQeErRvu0eb9hnbpMEf3YfWn6doCmEo/edit?usp=sharing) <br>
5. [Object detection models](https://docs.google.com/document/d/1oyGZHHzRHcmMOoldbjm7wCK3dkJ-7gtGof0N4dW9fVw/edit?usp=drive_link) <br>
6. [OCR models](https://docs.google.com/document/d/18qM2dFXHbgXnvP3ySgOd4ukTuyles6SsI28S-PcyeaI/edit?usp=sharing) <br>
7. Metrics <br>
7.1. [For object detection](https://docs.google.com/document/d/1gF7hUGuEaBBVK9QK3PbdQ7uVQ0yv3CMETL3OVLUlvEE/edit?usp=sharing) <br>
7.2. [For OCR](https://docs.google.com/document/d/1gbkllynV_VnRC7kU6yAdftyvBRl9nBQqga4lA6cOXVE/edit?usp=sharing) <br>




## Deep Learning

1. Hyperparameters <br>
1.1 [Read here about:](https://docs.google.com/document/d/1SeBsaNKqrJzQZrWqqRJ-dLbSdnaPPCKuTmHVk9sFgis/edit?usp=sharing)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input Shape,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hidden Layer Neurons,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dropout Rate,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Batch Size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Learning Rate,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number of Epochs,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Optimizer Parameters,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Regularization Parameters<br>
1.2. [Activation Functions](https://docs.google.com/document/d/1nuf9Sydn8D1g0hLCnY3duVTc_Aph2LiYAUl-sNA8oWQ/edit?usp=sharing) <br>
1.3. [Weight Initialization](https://docs.google.com/document/d/1NrKfeDi9v8HY42Bc1_ZL3S2QPSamg4e_j-4kt0ezMB4/edit?usp=sharing) <br>
1.4. [Loss Function](https://docs.google.com/document/d/1mTPMb2dCxIQXr5UlYbKbtTIDIvr_vOlVjF3lbYE64bw/edit?usp=sharing) <br>

3. Gradient descent, backpropagation, gradient exploding and dying problem --> https://www.youtube.com/watch?v=qowp6SQ9_Oo
   
4. Optimizers <br>
3.1. [Adam, bla, bla](https://docs.google.com/document/d/1gxzzkCKq473y-CbyVygYn_on2iq6VAUl7gvCNjx2jPw/edit?usp=sharing) <br>

5. [Regularization](https://docs.google.com/document/d/1ZGH61bgoCKa5myyzvbZPWwccbUwqF7E5mYg1q-YfmEU/edit?usp=sharing) <br>

6. [Batch Normalization](https://docs.google.com/document/d/1tg1jl9BvSU4bo2Gj50ChzWZxHCNAVdeq1ei-VxDb84E/edit?usp=sharing) <br>

7. Metrics <br>
6.1. [Accuracy](https://docs.google.com/document/d/19BU5OgnDCtSiLgijS1M26HLVhUYEh0nmCSY2twawXHQ/edit?usp=sharing) <br>
6.2. [Precision](https://docs.google.com/document/d/1EgYHTAErdFjcAzMgpTGq1EyKsTldvfXpaLY18kNPn8U/edit?usp=sharing) <br>
6.3. [Recall](https://docs.google.com/document/d/1hVPDBDxjs3zA5y8cP_iapi2LUEUVMl9FpfBIVaDmIL4/edit?usp=sharing) <br>
6.4. [F1](https://docs.google.com/document/d/1LaUgQAWQLVVXNLM0jcw0sgOkBbuVlpD0aeO2DkNqQ9I/edit?usp=sharing) <br>
6.5. [Additional Helpful Resource](https://docs.google.com/document/d/1IJWCtFApzoG66IYiUv5Rgxdz6RkWSavLGKUp_-dY6-c/edit?usp=sharing) - TP, FP, TN, FN, Confusion Matrix, macro-avg (P, R, F1), weighted-avg (P, R, F1) <br>

8. K-fold cross validation, what to do if model is overfitting or underfitting, what to do if we less training data (quickly check first if data you have is actually enough or you actually need more)
