## NLP

1. Architectures <br>
1.1. [RNN and LSTM (improved RNN arch)](https://docs.google.com/document/d/1al-YFsSfIWcVt9nRIfFREqJVszUKPduPXgtNyuxLaZA/edit?usp=sharing)<br>
1.2. Transformer (a type of Language model) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.2.1. [Vanilla Transformer](https://docs.google.com/document/d/1iM8tG3rspHcS-D23kZVo71uP3Qkm9HUyAOAXesK7rgc/edit?usp=sharing) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.2.2. [Transformer-based pre-trained LMs for machine translation](https://docs.google.com/document/d/1okp-PTvV5kAsaFU6yEpyZLIzcN9yDKP9WwesKo_uHw0/edit?usp=sharing) <br>
1.3. BERT (a type of Language model) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.3.1. [Original BERT](https://docs.google.com/document/d/1QzvKquupoa3m-oZXYKaf-Zp189hQoyp1oiDh5eZgjcs/edit?usp=drive_link) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.3.2. [BioBERT](https://docs.google.com/document/d/1YWCwVeTFuEtk6hyeW-iKhRIJGiyd6ZUqzBAh6R6ckW4/edit?usp=sharing) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.3.2. [RoBERTa](https://docs.google.com/document/d/12_b7DXJf7bDoZGO1IpvuKBYI7NGaTcg3PWaywq4Po3o/edit?usp=sharing) <br>

2. Word Embedding Models (https://www.linkedin.com/posts/amanc_artificialintelligence-machinelearning-deeplearning-activity-7152142197998424064-gxIh?utm_source=share&utm_medium=member_android)<br>
2.1. Context-independent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.1.1. Context-independent without ML <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - BOW<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - TF-IDF<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.1.2. Context-independent with ML <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [Word2Vec](https://docs.google.com/document/d/1uMI2jRvtdNcC7F-c9de-xiXpc__-u0TOcJnucnup-Vc/edit?usp=sharing) <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - [GloVe](https://docs.google.com/document/d/1h-s2ePP7vvNtX0noQXvpXr6_oTeKCFPdFmKBGq-y7ho/edit?usp=sharing) <br>
2.2. Context-independent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.1. Context-independent RNN-based [ELMO and CoVe] <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.2. Context-independent Transformer-based [BERT, roBERTa, ALBERT] <br>
  
5. Decoding algorithms <br>
3.1 [Greedy search vs Beam search](https://docs.google.com/document/d/1JrVWp7wnZP2rT4xRL-KTCtDI5hdW_jniup_iFwdP3To/edit?usp=sharing) <br>

6. Metrics <br>
4.1. [BLEU score](https://docs.google.com/document/d/1lKH2x3n77tTvh3Jfe6sV-VisUQZ68Q0YaD3WQSW13kk/edit?usp=sharing) <br>
4.2. [ROUGE score](https://docs.google.com/document/d/1xUQj_GsOtHkqW8wq5NAYLoqvkfCqhXfFGsAqX9ZEsag/edit?usp=sharing) <br>
4.3. [Character Error Rate (CER) & Word Error Rate (WER)](https://docs.google.com/document/d/1XbUNDnR6FfuDVmdKZopTqz5a5UWmyCzfso-Hn73p7Ro/edit?usp=drive_link) <br>
4.4. [Cosine Similarity]() 




## Computer Vision 

1. [Convolutional Neural Networks](https://docs.google.com/document/d/1vEKRSIQn7QWPnfpE1zpILDuuzjyD0k33RLU7N--niSM/edit?usp=sharing) -- arch, working, accuracies <br>
2. [Background extraction algorithms](https://docs.google.com/document/d/1XEhd3OpThcv1u9DNhny6Gjk2W1ihsylnhBf5Q4bcbks/edit?usp=sharing) <br>
3. [Face detection, alignment, and recognition models](https://docs.google.com/document/d/1trH2mB0tfAA9cAvHFnjXmSHT_wx5Xs4iKmy7ShQOOw8/edit?usp=sharing) <br>
4. [Similarity metrics for finding similarity between image representation vectors](https://docs.google.com/document/d/1BU7CyktUr5x3UQeErRvu0eb9hnbpMEf3YfWn6doCmEo/edit?usp=sharing) <br>
5. [Object detection models](https://docs.google.com/document/d/1oyGZHHzRHcmMOoldbjm7wCK3dkJ-7gtGof0N4dW9fVw/edit?usp=drive_link) <br>
6. [OCR models](https://docs.google.com/document/d/18qM2dFXHbgXnvP3ySgOd4ukTuyles6SsI28S-PcyeaI/edit?usp=sharing) <br>
7. Metrics <br>
7.1. [For object detection](https://docs.google.com/document/d/1gF7hUGuEaBBVK9QK3PbdQ7uVQ0yv3CMETL3OVLUlvEE/edit?usp=sharing)
8. Loss Function <br>
8.1. [For object detection](https://docs.google.com/document/d/1mOHXKfUgkNBlHHB59gjY9lV5ZZkygeVyc4XMkXY7G2g/edit?usp=sharing) 


## Deep Learning

1. Into about Deep Learning https://www.nirajai.com/home/deep-learning#h.zbtkd5gqp8w , https://www.youtube.com/watch?v=ra6jYAdeXSI&list=PLehuLRPyt1HxuYpdlW4KevYJVOSDG3DEz&index=2&pp=iAQB<br>

2. Hyperparameters <br>
2.1 [Read here about:](https://docs.google.com/document/d/1SeBsaNKqrJzQZrWqqRJ-dLbSdnaPPCKuTmHVk9sFgis/edit?usp=sharing)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Input Shape,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hidden Layer and Neurons in Hidden Layer,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dropout Rate,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Learning Rate,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Batch Size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Number of Epochs<br>
2.2. [Activation Functions](https://docs.google.com/document/d/1nuf9Sydn8D1g0hLCnY3duVTc_Aph2LiYAUl-sNA8oWQ/edit?usp=sharing) <br>
2.3. [Weight Initialization](https://docs.google.com/document/d/1NrKfeDi9v8HY42Bc1_ZL3S2QPSamg4e_j-4kt0ezMB4/edit?usp=sharing) <br>
2.4. [Loss Function](https://docs.google.com/document/d/1mTPMb2dCxIQXr5UlYbKbtTIDIvr_vOlVjF3lbYE64bw/edit?usp=sharing) <br>

3. Backpropagation, Gradient descent (stochastic GD, batch GD, and mini-batch GD), and exploding gradient & vanishing gradient problems
   
4. Optimizers <br>
4.1. [Adam, bla, bla](https://docs.google.com/document/d/1gxzzkCKq473y-CbyVygYn_on2iq6VAUl7gvCNjx2jPw/edit?usp=sharing) <br>

5. [Regularization](https://docs.google.com/document/d/1ZGH61bgoCKa5myyzvbZPWwccbUwqF7E5mYg1q-YfmEU/edit?usp=sharing) <br>

6. [Batch Normalization](https://docs.google.com/document/d/1tg1jl9BvSU4bo2Gj50ChzWZxHCNAVdeq1ei-VxDb84E/edit?usp=sharing) <br>

7. Metrics <br>
7.1. [Read here about:](https://docs.google.com/document/d/19BU5OgnDCtSiLgijS1M26HLVhUYEh0nmCSY2twawXHQ/edit?usp=sharing)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Accuracy,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Precision,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Recall,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Precision-Recall Curve,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F1<br>
7.2. [Read here about:](https://docs.google.com/document/d/1IJWCtFApzoG66IYiUv5Rgxdz6RkWSavLGKUp_-dY6-c/edit?usp=sharing)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TP,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FP,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TN,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FN,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Confusion Matrix,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Macro-avg P, R and F1,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Weighted-avg P, R and F1<br>

8. [Read here about: ](https://docs.google.com/document/d/1gRNhomZx-FmHJzawa7fsLk3iKcx4c_-in-UaH0pO5X0/edit?usp=sharing)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8.1. Bias & variance tradeoff<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8.2. Relation between Bias & Variance and Overfitting, Underfitting and Optimal model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8.3. Cross-validation technique (aka validation process done after model training) and its types to avoid model overfitting<br>

