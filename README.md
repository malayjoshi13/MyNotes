## NLP

1. Architectures <br>
1.1. [LSTM (a type of RNN)](https://docs.google.com/document/d/1al-YFsSfIWcVt9nRIfFREqJVszUKPduPXgtNyuxLaZA/edit?usp=sharing): see the code to use this architecture for task of Image Captioning [here](https://github.com/malayjoshi13/Describer). <br>
1.2. Transformer (a type of Language model) <br>
1.2.1. [Vanilla Transformer](https://docs.google.com/document/d/1VDVUjCs7R7CApmPvXT95OdPNIed1u_8QavRHpIvRL1M/edit?usp=sharing): see the code to implement it from scratch [here](https://github.com/malayjoshi13/Understanding-Transformer). <br>
1.2.2. [Transformer-based pre-trained LMs for machine translation (Helsinki-NLP/opus-mt-hi-en, facebook/mbart-large-50-many-to-one-mmt, facebook/m2m100_418M)](https://docs.google.com/document/d/1okp-PTvV5kAsaFU6yEpyZLIzcN9yDKP9WwesKo_uHw0/edit?usp=sharing): see the code to use these models for task of Machine Translation [here](https://github.com/malayjoshi13/NeuralMachineTranslator). <br>

2. Word Embedding Models <br>
2.1. [GloVe](https://docs.google.com/document/d/1h-s2ePP7vvNtX0noQXvpXr6_oTeKCFPdFmKBGq-y7ho/edit?usp=sharing): used [here](https://github.com/malayjoshi13/Describer). <br>
2.2. [Word2Vec](https://docs.google.com/document/d/1uMI2jRvtdNcC7F-c9de-xiXpc__-u0TOcJnucnup-Vc/edit?usp=sharing)<br>

3. Decoding algorithms <br>
3.1 [Greedy search vs Beam search](https://docs.google.com/document/d/1JrVWp7wnZP2rT4xRL-KTCtDI5hdW_jniup_iFwdP3To/edit?usp=sharing): used [here](https://github.com/malayjoshi13/Describer). <br>

4. Metrics <br>
4.1. [BLEU score](https://docs.google.com/document/d/1lKH2x3n77tTvh3Jfe6sV-VisUQZ68Q0YaD3WQSW13kk/edit?usp=sharing): used [here](https://github.com/malayjoshi13/Describer) and [here](https://github.com/malayjoshi13/NeuralMachineTranslator). <br>
4.2. [ROUGHE score](https://docs.google.com/document/d/1xUQj_GsOtHkqW8wq5NAYLoqvkfCqhXfFGsAqX9ZEsag/edit?usp=sharing) <br>
4.3. [Character Error Rate](https://docs.google.com/document/d/1XbUNDnR6FfuDVmdKZopTqz5a5UWmyCzfso-Hn73p7Ro/edit?usp=drive_link) <br>
4.4. [Word Error Rate](https://docs.google.com/document/d/1FVwdXAT_zKgrG4McqE8_TYUM1b4zRimPRfiPkmRSnFA/edit?usp=drive_link) <br>




## Computer Vision 

1. [Convolutional Neural Networks](https://docs.google.com/document/d/1vEKRSIQn7QWPnfpE1zpILDuuzjyD0k33RLU7N--niSM/edit?usp=sharing) -- arch, working, accuracies:  used [here](https://github.com/malayjoshi13/Describer)




## Deep Learning

1. Hyperparameters <br>
1.1 [Input Shape, Hidden Layer Neurons, Dropout Rate, Batch Size, Learning Rate, Number of Epochs, Optimizer Parameters, Regularization Parameters, Batch Normalization Parameters](https://docs.google.com/document/d/1SeBsaNKqrJzQZrWqqRJ-dLbSdnaPPCKuTmHVk9sFgis/edit?usp=sharing) <br>
1.2. [Activation Functions, Weight Initialization, Loss Function](https://docs.google.com/document/d/1nuf9Sydn8D1g0hLCnY3duVTc_Aph2LiYAUl-sNA8oWQ/edit?usp=sharing):  ReLU, Softmax and Categorical cross entropy used [here](https://github.com/malayjoshi13/Describer). <br>

2. Optimizers <br>
2.1. [Adam, bla, bla](https://docs.google.com/document/d/1gxzzkCKq473y-CbyVygYn_on2iq6VAUl7gvCNjx2jPw/edit?usp=sharing): Adam used [here](https://github.com/malayjoshi13/Describer). <br>

3. [Regularization](https://docs.google.com/document/d/1ZGH61bgoCKa5myyzvbZPWwccbUwqF7E5mYg1q-YfmEU/edit?usp=sharing) <br>

4. [Batch Normalization](https://docs.google.com/document/d/1tg1jl9BvSU4bo2Gj50ChzWZxHCNAVdeq1ei-VxDb84E/edit?usp=sharing) <br>

